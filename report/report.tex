\documentclass[12pt]{article}

\usepackage{amssymb}

\begin{document}

% Title page

\begin{titlepage}
    \begin{center}
        \vspace*{2in}

        \Huge
        An Interesting Title

        \vspace*{2in}

        \Large

        Trinity Term 2022

        \vspace*{0.25in}
        Candidate Number: \emph{1034710}

        \vspace*{0.25in}
        Masters in Computer Science
    \end{center}
\end{titlepage}



\begin{center}
    \large \textbf{Abstract}
\end{center}

This project concerns building a proof-of-concept for a general-purpose, non-destructive,
GPU-accelerated image editor.  Let's break down exactly what these goals mean:

Firstly, the image editor must be \emph{general purpose}.  This means that the user isn't restricted
to a specific work flow (like adjusting photos)---the image editor must have a flexible internal
representation that allows the user to creatively manipulate images in whatever way they want.

Secondly, all image filters must be \emph{non-destructive}.  A `destructive' image filter is one
which, when applied to a layer, overwrites the source layer.  Thus, the source layer is destroyed.
From a user experience (often shortened to UX) perspective, this is very undesirable; if, for
example, you were to apply a blur followed by colour correction, there would be no way to adjust the
blur because the source layer has been destroyed by both the blur and the colour correction.

If image filters are non-destructive, then it has to be the case that the user can change an filter
at the bottom of a stack of filters and the editor must recompute all the filters above it.  For the
ideal user experience, this must all be processed within a frame so that the delay isn't
perceptible.  To target 60 frames per second, an editor must process every update in less than
16.6ms---including processing and rendering the GUI.  The only way to realistically achieve such
low latencies is to do \emph{all} the image processing on the GPU.  Thus, our editor is
\emph{GPU-accelerated}.



\pagebreak

\tableofcontents



\pagebreak

\section{Introduction}

When image editing, the user's mental model of an edited image corresponds to a sequence of layers,
which are conceptually source.  Each layer can be passed through any number of `filters', which
modify these layers.  The `filtered' version of the layers are finally merged (or `composited')
together to form the final `composite' image.

The ideal user experience for an image editor is one where the editor directly exposes precisely
this model and, when the user makes changes, instantly updates the image in the screen.  Here
`instant' means that the update latency is imperceptible, ideally within 16.6ms so that the new
image can be rendered in the same frame as the UI updates (assuming 60 frames per second).

Existing programs like Adobe Photoshop and Darktable shows us that fully non-destructive editing is
entirely possible, but recomputing the image within a frame requires an enormous amount of
computation.  Fortunately, almost all modern consumer computers contain extremely capable graphics
processing units (GPUs), which are designed to be extremely fast at highly parallel workloads such
as image processing.  Even better, video game developers have spent decades working on minimising
latency of GPU operations.  Image processing is very computationally intensive but embarrassingly
parallel, so, to perform this much computation with low latency, I propose running \emph{all} of the
compute work on the same GPU that drives the user's display.

This project aims to determine the feasibility of a fully non-destructive, fully GPU-accelerated
image editor.  To do so, I have implemented a simple `core' of an image editor, and used this to
measure the latency of full GPU processing.  There are many further optimisations that could be
done, but they will not change the performance enough to change the feasibility result (TODO: Update
this once I actually have results).

TODO: Results



\pagebreak

\section{Background}

\subsection{Image Processing 101}

Image editors will largely be performing one of three types of operation:

\begin{enumerate}
    \item Per-pixel filters, where each pixel in the output is a simple function of either the same
        pixel in the input (e.g.\ brightness/contrast, hue/saturation/value) or the pixels near it
        (e.g.\ pick/hurl noise, edge detect/sharpen, small blurs).
    \item Transformations, where the image is translated, rotated or distorted in some way.
    \item Compositing, where a large number of `layers' are merged to form one final image.
\end{enumerate}

All three of these are embarrassingly parallel---in each case, every pixel of the output can be
computed independently to all the others.

There are some image filters (like the various kinds of blur) which aren't as obviously
parallelisable, but there is still parallelisation opportunities (like computing 2D Gaussian blur by
combining horizontal and vertical blur passes, where each row/column is independent).



\subsection{Methods of Processing Images}

In a typical consumer computer, there are two main computation units which could be used for
processing images:

\subsubsection{The Central Processing Unit (CPU)}

Every computer contains a main processor (the CPU), which is optimised for performing general
computations on the same data.  There is some parallelisation available (according to Steam's
Hardware Survey, CPUs have an average of 4.9 cores, as of April 2022), but these are optimised for
fast sequential processing rather than high parallelism.

Because of this, it's obvious that the CPU is a pretty inappropriate device for image processing.
The saving grace is that CPUs tend to have extremely high clock speeds and some degree of
parallelism: with multiple cores and SIMD, can perform upwards of 32 individual operations in
parallel\footnote{This is assuming four cores, each of which can perform operations on eight 32-bit
floating-point SIMD lanes in a 256-bit AVX register.  However, limitations like memory speed and the
sharing of functional units (thus causing logically different threads to contest for the same
hardware) make this theoretical limit very difficult to reach in practice.}.  This is fast enough
that CPU-based processing is \emph{passable} but the latency and throughput are both far from ideal.
Additionally, other CPU-bound tasks (including GUI layout) all take time away from the image
processing.

\subsubsection{The Graphics Processing Unit (GPU)}

Almost all modern computers also contain a secondary processor, the Graphics Processing Unit (GPU),
who's sole purpose is to accelerate graphics processing with a low latency and high throughput.

Some form of GPU is present in nearly every consumer device, with even low-powered mobile devices
featuring GPUs integrated into the same piece of silicon as the CPU.  Therefore, we can confidently
rely on graphics acceleration being present when writing consumer applications.  Note that this has
only recently been the case---over the last two decades, rising screen resolutions and refresh
rates made dedicated graphics processing go from exclusive to gaming to being ubiquitous.

GPUs have two major processing modes:

\paragraph{Render passes:} Typically, a render pass involves taking 3D geometry, applying some
arbitrary transformation to the vertices, then performing some computation on every pixel of the
screen to compute its colour.  This is what GPUs are primarily designed for (since it's what games
need), and is therefore GPUs have had many decades of optimisation relating to rendering
efficiently.  Because rendering is a GPU's primary purpose, it means that render passes are
available on any GPU.  Render passes are thus extremely portable.

\paragraph{Compute passes:} More recently, the raw power of GPUs has made people want to use them as
general purpose computing devices.  Any highly parallel tasks are very well suited to GPU
acceleration, including machine learning and (in our case) image processing.  Therefore, recent GPUs
can run arbitrary code on the same cores that would be used for rendering.  This has given rise to
General Purpose GPU (GPGPU) computing, which heavily utilises this ability.  Compute passes are
useful for doing operations that can't easily be represented as per-pixel operations, like
simulations or more complex filters such as Gaussian blur.

\subsection{Comparison of Compute APIs}

\subsubsection{CPU Compute (No API Needed)}

Computing images on the CPU is extremely simple, requiring no extra effort or clever APIs.  Source
images are loaded from disk into the CPU's memory as an array of pixel values.  Operations can then
be performed on those pixels using a plain \verb|for| loop.  Parallelisation can be achieved
using a combination of SIMD and multi-threading (splitting the pixels into chunks and having
different CPU cores process those chunks in parallel).  As described above, CPUs are easy to program
for but are quite ill-suited to the heavily parallel operations found in image processing.

\subsubsection{Graphics Libraries}

Directly controlling GPUs is clearly an unsustainable task, since you would be forced to deal with
the differences between every GPU.  To avoid this issue, operating systems vendors (like Apple and
Microsoft) or the Khronos Group\footnote{https://www.khronos.org/} create APIs which provide users
with a simpler abstraction for the GPU, and require that GPU vendors implement that API in the GPU's
drivers.

The oldest and most well-known graphics API is OpenGL\footnote{https://www.opengl.org/}.  OpenGL is
very high-level, which makes it easy to use for simple cases (like making games) but lacks the
low-level control required for achieving high performance in more obscure cases (like image
editors).

To overcome this, Khronos released Vulkan\footnote{https://www.vulkan.org/} as a successor to
OpenGL, whilst Apple released Metal\footnote{https://developer.apple.com/metal/} and Microsoft
released DirectX\footnote{https://en.wikipedia.org/wiki/DirectX}.  All of these favour a lower-level
approach, requiring more code from their users but allowing a much higher performance ceiling.

Until 2018, OpenGL and Vulkan where fully supported by all major operating systems (Windows, macOS,
Linux, iOS and Android).  In 2018, Apple announced that they would be deprecating OpenGL (along with
Vulkan and OpenCL) to encourage developers to use Metal.  As of 2022, OpenGL, Vulkan and OpenCL all
still work on Apple devices, but it's unclear for how long this will continue.  Code using Vulkan
can be used on Apple devices by using MoltenVK\footnote{https://github.com/KhronosGroup/MoltenVK} to
translate Vulkan function calls into Metal at runtime.  Similarly,
MoltenGL\footnote{https://moltengl.com/moltengl/} provides support for OpenGL, but both of these
only implement a subset of their corresponding APIs and are less than ideal.  The compatibility of
various APIs, along with OpenCL (discussed in Section~\ref{sec:open-cl}) and WebGPU (discussed in
Section~\ref{sec:wgpu}), is summarised in Figure~\ref{fig:apis-vs-oses}.

\begin{figure}
    \begin{center}
        \begin{tabular}{ c | c c c c c }
                    & Windows & macOS & Linux & iOS & Android \\
            \hline
            OpenCL  & \checkmark & deprecated   & \checkmark & deprecated   & \checkmark \\
            OpenGL  & \checkmark & subset       & \checkmark & subset       & \checkmark \\
            Vulkan  & \checkmark & subset       & \checkmark & subset       & \checkmark \\
            Metal   &            & \checkmark   &            & \checkmark \\
            DirectX & \checkmark \\
            \hline
            WebGPU & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark
        \end{tabular}
    \end{center}
    \caption{Compatibility table of APIs against major operating systems.  iOS and Android are less
       relevant to image editors, but are included for completeness.}\label{fig:apis-vs-oses}
\end{figure}

\subsubsection{OpenCL}\label{sec:open-cl}

OpenCL\footnote{https://www.khronos.org/opencl/} is a framework for running general compute
operations on any hardware, again standardised by the Khronos Group.  OpenCL lets you
write C or C++ code once, then delegate that code to any compute device at runtime.  Thus, the
compute code only needs to be written once.

Apple has also deprecated OpenCL, but, as far as I'm aware, no one has yet made a compatibility
layer like MoltenVK or MoltenGL.  Also, OpenCL always uses compute passes, potentially missing out
on performance in tasks where render passes would be much faster (such as compositing).

\subsubsection{WebGPU}\label{sec:wgpu}

WebGPU\footnote{https://www.w3.org/TR/webgpu/} is an in-progress standard to give web applications
low-level but safe access to GPU acceleration.  WebGPU implements a common subset of Vulkan, Metal
and DirectX to give it extremely good portability.  A Rust library,
\verb|wgpu|\footnote{https://github.com/gfx-rs/wgpu}, implements the WebGPU specification in a way
callable from languages other than JavaScript.  \verb|wgpu| forms the core of Firefox's WebGPU
implementation, and is therefore extremely well-tested and stable.



\pagebreak

\section{State of the Art} % TODO: Should I call this 'Existing Editors'

\subsection{Adobe Photoshop}

Used by an estimated 26 million users
world-wide\footnote{https://prodesigntools.com/number-of-creative-cloud-subscribers.html}, including
90\% of the world's creative
professionals\footnote{https://www.adobe.com/about-adobe/fast-facts.html}, Adobe
Photoshop\footnote{https://www.adobe.com/uk/products/photoshop.html} is the
undisputed king of image editing.

\subsubsection{Non-Destructive Editing}

Since the introduction of Smart
Objects\footnote{https://helpx.adobe.com/photoshop/using/create-smart-objects.html} (and the
corresponding Smart Filters), Photoshop has been able to implement most filters non-destructively.
These are all computed on the CPU, though, so extensive use of Smart Filters will often cause
Photoshop to noticeably slow down.

\subsubsection{GPU Acceleration}

Photoshop runs almost all of its image processing on the CPU.  As of 2022, nine operations benefit from
GPU acceleration, whilst a further eight require a
GPU\footnote{https://prodesigntools.com/number-of-creative-cloud-subscribers.html}.  It makes sense
that the Photoshop is not built with GPUs in mind---the first version of Photoshop was released in
1990\footnote{https://en.wikipedia.org/wiki/Adobe\_Photoshop}, nine years before the first consumer
GPU (Nvidia's GeForce 256) reached the market\footnote{https://en.wikipedia.org/wiki/GeForce\_256}.

Without access to Photoshop's source code, it's unclear exactly what compute APIs are being used for
these GPU-accelerated filters.  Photoshop has settings for OpenCL, OpenGL, Vulkan and Metal, so I
can only assume that a combination of all these are used.  So, in summary, Photoshop appears to use
largely CPU, with a small amount of GPU compute passes and GPU rendering passes.

\subsection{GNU Image Manipulation Program (GIMP)}

The GNU Image Manipulation Program\footnote{https://gimp.org} (shortened to GIMP) is a free and open
source general-purpose image editor.

\subsubsection{Non-Destructive Editing}

As of 2022, all filters in GIMP are destructive, so there is no way to do non-destructive editing.
There is a plan to implement non-destructive
editing\footnote{https://www.gimp.org/docs/userfaq.html\#when-will-gimp-support-any-kind-of-non-destructive-editing-like-adjustment-layers-layer-filters-andor-full-blown-node-based-editing}
but that has not yet come to fruition.  This appears to be a really huge task, because GIMP is not
written with non-destructive editing in mind.

\subsubsection{GPU Acceleration}

GIMP has experimental support for GPU acceleration via OpenCL (first released in
2.10.0\footnote{https://www.gimp.org/news/2018/04/27/gimp-2-10-0-released/} in April 2018).  As of
2022, this is still experimental and is sometimes slower than CPU-based processing.  So, for most
users, GIMP is 100\% CPU-bound.

\subsection{Darktable}

Darktable\footnote{https://www.darktable.org/} is an open source `photography workflow application'
which features non-destructive editing which can be performed entirely on the GPU.  The only thing
really missing is that Darktable only supports a fixed pipeline of filters, rather than allowing the
user to choose how to compose them (this makes sense, because Darktable is only intended to be used
for adjusting photos).  Finally, Darktable uses OpenCL which, as described above in
Section~\ref{sec:open-cl}, may miss out on performance improvements from using GPU render passes.




\section{Implementation}



\section{Measurement Methodology}

\section{Results}

\section{Conclusion}



\pagebreak

\begin{thebibliography}{99}
\end{thebibliography}

\end{document}
